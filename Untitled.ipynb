{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d3696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36efe766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\srija\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\srija\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\srija\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\srija\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\srija\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "import string \n",
    "punctuations =string .punctuation \n",
    "\n",
    "# for retaining the names of places in the \"text\" feature\n",
    "from geotext import GeoText\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "086d71a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f25f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b605c96c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bce02bc7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3b0e80",
   "metadata": {},
   "source": [
    "### Missing value imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b107dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df[\"keyword\"].unique())\n",
    "# print(df[\"location\"].unique())\n",
    "df[\"location\"].fillna(\"\",inplace=True)\n",
    "df[\"keyword\"].fillna(\"\",inplace=True)\n",
    "# there are no missing values now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "e1a6a23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Our Deeds are the Reason of this #earthquake M...\n",
       "1                  Forest fire near La Ronge Sask. Canada\n",
       "2       All residents asked to 'shelter in place' are ...\n",
       "3       13,000 people receive #wildfires evacuation or...\n",
       "4       Just got sent this photo from Ruby #Alaska as ...\n",
       "                              ...                        \n",
       "7608    Two giant cranes holding a bridge collapse int...\n",
       "7609    @aria_ahrary @TheTawniest The out of control w...\n",
       "7610    M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...\n",
       "7611    Police investigating after an e-bike collided ...\n",
       "7612    The Latest: More Homes Razed by Northern Calif...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2931cabb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a0e2fa0fa53b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# word_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# unique_word_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# word_count\n",
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "test['word_count'] = test['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# unique_word_count\n",
    "df['unique_word_count'] = df['text'].apply(lambda x: len(set(str(x).split())))\n",
    "test['unique_word_count'] = test['text'].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "# stop_word_count\n",
    "STOPWORDS=stopwords.words('english')\n",
    "df['stop_word_count'] = df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "test['stop_word_count'] = test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "\n",
    "# url_count\n",
    "df['url_count'] = df['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "test['url_count'] = test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "\n",
    "# mean_word_length\n",
    "df['mean_word_length'] = df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test['mean_word_length'] = test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "# char_count\n",
    "df['char_count'] = df['text'].apply(lambda x: len(str(x)))\n",
    "test['char_count'] = test['text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# punctuation_count\n",
    "df['punctuation_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "test['punctuation_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "# hashtag_count\n",
    "df['hashtag_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "test['hashtag_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "\n",
    "# mention_count\n",
    "df['mention_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
    "test['mention_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "469624cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cols = [\"word_count\",\"unique_word_count\",\"stop_word_count\",\"url_count\",\"mean_word_length\",\"char_count\",\"punctuation_count\",\"hashtag_count\",\"mention_count\"]\n",
    "# fig, ax = plt.subplots(len(cols),1,figsize=(10,6*len(cols)))\n",
    "# j=0\n",
    "# for i in cols:\n",
    "#     (sns.countplot(x=i, hue=\"target\", data=df,ax=ax[j]))\n",
    "#     j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2662bf",
   "metadata": {},
   "source": [
    "### Duplicated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc321747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7503, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.drop_duplicates(subset=\"text\",keep=\"first\", inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "4efab907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.catplot(x=\"target\",y=\"number_of_words\",data=df)\n",
    "# # therefore of no use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053131d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "800e705c",
   "metadata": {},
   "source": [
    "## Applying Filters to clean text\n",
    "### Removing non alphanumerics and non special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2df9a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# re.sub(\"[^a-zA-Z0-9!@#$&()\\\\- :.+,/\\\"]\", \"\", df[\"text\"].iloc[38]),df[\"text\"].iloc[38]\n",
    "\n",
    "# ascii chracter filtered, mentions, urls\n",
    "df[\"clean_text\"]=df[\"text\"].apply(lambda x: re.sub(\"[^a-zA-Z0-9!@$&()\\\\- :.+,/\\\"]|@([A-Za-z]+[A-Za-z0-9-_]+)|http([A-Za-z0-9-_.:?/]+)\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da815294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive wildfires evacuation ord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7604</th>\n",
       "      <td>10863</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#WorldNews Fallen powerlines on G:link tram: U...</td>\n",
       "      <td>1</td>\n",
       "      <td>WorldNews Fallen powerlines on G:link tram: UP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>10864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>on the flip side I'm at Walmart and there is a...</td>\n",
       "      <td>1</td>\n",
       "      <td>on the flip side Im at Walmart and there is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7606</th>\n",
       "      <td>10866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Suicide bomber kills 15 in Saudi security site...</td>\n",
       "      <td>1</td>\n",
       "      <td>Suicide bomber kills 15 in Saudi security site...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7503 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7604  10863     NaN      NaN   \n",
       "7605  10864     NaN      NaN   \n",
       "7606  10866     NaN      NaN   \n",
       "7608  10869     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7604  #WorldNews Fallen powerlines on G:link tram: U...       1   \n",
       "7605  on the flip side I'm at Walmart and there is a...       1   \n",
       "7606  Suicide bomber kills 15 in Saudi security site...       1   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "                                             clean_text  \n",
       "0     Our Deeds are the Reason of this earthquake Ma...  \n",
       "1                Forest fire near La Ronge Sask. Canada  \n",
       "2     All residents asked to shelter in place are be...  \n",
       "3     13,000 people receive wildfires evacuation ord...  \n",
       "4     Just got sent this photo from Ruby Alaska as s...  \n",
       "...                                                 ...  \n",
       "7604  WorldNews Fallen powerlines on G:link tram: UP...  \n",
       "7605  on the flip side Im at Walmart and there is a ...  \n",
       "7606  Suicide bomber kills 15 in Saudi security site...  \n",
       "7608  Two giant cranes holding a bridge collapse int...  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...  \n",
       "\n",
       "[7503 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(df[\"ascii_text\"].iloc[38],df[\"text\"].iloc[38])\n",
    "# print(df.iloc[7609][\"text\"],df.iloc[7609][\"no_mentions_text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "25d1571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to change this to key: comedian, value: string format\n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac271ef",
   "metadata": {},
   "source": [
    "### Function to remove non english words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1a3cae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_in_english_dictionary(list_of_text):\n",
    "    '''Function which takes list of words as arguement and return list of only words present in english dictionary.'''\n",
    "    word_list=[]\n",
    "    for word in list_of_text:\n",
    "        if d.check(word)==True:\n",
    "            word_list.append(word)\n",
    "    combined_text = ' '.join(word_list)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d71746",
   "metadata": {},
   "source": [
    "### Step - a : Remove blank rows if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dea91dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>The out of control wild fires in California ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>M1.94 01:04 UTC5km S of Volcano Hawaii.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   \n",
       "7611  Police investigating after an e-bike collided ...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "                                             clean_text  \n",
       "0     Our Deeds are the Reason of this #earthquake M...  \n",
       "1                Forest fire near La Ronge Sask. Canada  \n",
       "2     All residents asked to shelter in place are be...  \n",
       "3     13,000 people receive #wildfires evacuation or...  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...  \n",
       "...                                                 ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...  \n",
       "7609    The out of control wild fires in California ...  \n",
       "7610           M1.94 01:04 UTC5km S of Volcano Hawaii.   \n",
       "7611  Police investigating after an e-bike collided ...  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...  \n",
       "\n",
       "[7613 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'].dropna(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b9ea75ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, keyword, location, text, target, clean_text]\n",
       "Index: []"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"clean_text\"]==\"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21536752",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4924ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Our, Deeds, are, the, Reason, of, this, #, ea...\n",
       "1        [Forest, fire, near, La, Ronge, Sask, ., Canada]\n",
       "2       [All, residents, asked, to, shelter, in, place...\n",
       "3       [13,000, people, receive, #, wildfires, evacua...\n",
       "4       [Just, got, sent, this, photo, from, Ruby, #, ...\n",
       "                              ...                        \n",
       "7608    [Two, giant, cranes, holding, a, bridge, colla...\n",
       "7609    [The, out, of, control, wild, fires, in, Calif...\n",
       "7610    [M1.94, 01:04, UTC5km, S, of, Volcano, Hawaii, .]\n",
       "7611    [Police, investigating, after, an, e-bike, col...\n",
       "7612    [The, Latest, :, More, Homes, Razed, by, North...\n",
       "Name: clean_text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[\"clean_text\"] = [word_tokenize(entry) for entry in df['clean_text']]\n",
    "# df[\"clean_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36c8f23",
   "metadata": {},
   "source": [
    "### Applying function to remove non english words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98d091d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Our Deeds are the Reason of this earthquake Ma...\n",
       "1                       Forest fire near La Sask . Canada\n",
       "2       All residents asked to in place are being noti...\n",
       "3       13,000 people receive wildfires evacuation ord...\n",
       "4       Just got sent this photo from Ruby Alaska as s...\n",
       "                              ...                        \n",
       "7604    Fallen on G link tram UPDATE FIRE crews have e...\n",
       "7605    on the flip side I at Walmart and there is a b...\n",
       "7606    Suicide bomber kills 15 in Saudi security site...\n",
       "7608    Two giant cranes holding a bridge collapse int...\n",
       "7612    The Latest More Homes Razed by Northern Califo...\n",
       "Name: english_text, Length: 7503, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[\"english_text\"] = df[\"clean_text\"].apply(lambda x:word_in_english_dictionary(x))\n",
    "# df[\"english_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "33003c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb74ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df[\"english_text\"]==\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaed72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty_text = df[df[\"text\"]==\"\"].index\n",
    "# df.drop(empty_text,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f59db55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, ., canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>[13,000, people, receive, wildfires, evacuatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7604</th>\n",
       "      <td>#WorldNews Fallen powerlines on G:link tram: U...</td>\n",
       "      <td>[worldnews, fallen, powerlines, on, g, :, link...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>on the flip side I'm at Walmart and there is a...</td>\n",
       "      <td>[on, the, flip, side, im, at, walmart, and, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7606</th>\n",
       "      <td>Suicide bomber kills 15 in Saudi security site...</td>\n",
       "      <td>[suicide, bomber, kills, 15, in, saudi, securi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>[two, giant, cranes, holding, a, bridge, colla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>[the, latest, :, more, homes, razed, by, north...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7503 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...   \n",
       "1                Forest fire near La Ronge Sask. Canada   \n",
       "2     All residents asked to 'shelter in place' are ...   \n",
       "3     13,000 people receive #wildfires evacuation or...   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...   \n",
       "...                                                 ...   \n",
       "7604  #WorldNews Fallen powerlines on G:link tram: U...   \n",
       "7605  on the flip side I'm at Walmart and there is a...   \n",
       "7606  Suicide bomber kills 15 in Saudi security site...   \n",
       "7608  Two giant cranes holding a bridge collapse int...   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...   \n",
       "\n",
       "                                             clean_text  \n",
       "0     [our, deeds, are, the, reason, of, this, earth...  \n",
       "1      [forest, fire, near, la, ronge, sask, ., canada]  \n",
       "2     [all, residents, asked, to, shelter, in, place...  \n",
       "3     [13,000, people, receive, wildfires, evacuatio...  \n",
       "4     [just, got, sent, this, photo, from, ruby, ala...  \n",
       "...                                                 ...  \n",
       "7604  [worldnews, fallen, powerlines, on, g, :, link...  \n",
       "7605  [on, the, flip, side, im, at, walmart, and, th...  \n",
       "7606  [suicide, bomber, kills, 15, in, saudi, securi...  \n",
       "7608  [two, giant, cranes, holding, a, bridge, colla...  \n",
       "7612  [the, latest, :, more, homes, razed, by, north...  \n",
       "\n",
       "[7503 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"text\",\"clean_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583cbf9a",
   "metadata": {},
   "source": [
    "### Step - b : Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67d18bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = [entry.lower() for entry in df['clean_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621a174a",
   "metadata": {},
   "source": [
    "### Step - c : Tokenization : In this each entry in the df will be broken into set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ced9e0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [our, deeds, are, the, reason, of, this, earth...\n",
       "1        [forest, fire, near, la, ronge, sask, ., canada]\n",
       "2       [all, residents, asked, to, shelter, in, place...\n",
       "3       [13,000, people, receive, wildfires, evacuatio...\n",
       "4       [just, got, sent, this, photo, from, ruby, ala...\n",
       "                              ...                        \n",
       "7604    [worldnews, fallen, powerlines, on, g, :, link...\n",
       "7605    [on, the, flip, side, im, at, walmart, and, th...\n",
       "7606    [suicide, bomber, kills, 15, in, saudi, securi...\n",
       "7608    [two, giant, cranes, holding, a, bridge, colla...\n",
       "7612    [the, latest, :, more, homes, razed, by, north...\n",
       "Name: clean_text, Length: 7503, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text']= [word_tokenize(entry) for entry in df['clean_text']]\n",
    "df[\"clean_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1371999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4cb5f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [our, deeds, are, the, reason, of, this, earth...\n",
       "1               [forest, fire, near, la, sask, ., canada]\n",
       "2       [all, residents, asked, to, in, place, are, be...\n",
       "3       [13,000, people, receive, wildfires, evacuatio...\n",
       "4       [just, got, sent, this, photo, from, ruby, ala...\n",
       "                              ...                        \n",
       "7608    [two, giant, cranes, holding, a, bridge, colla...\n",
       "7609    [the, out, of, control, wild, fires, in, calif...\n",
       "7610                     [utc, s, of, volcano, hawaii, .]\n",
       "7611    [police, investigating, after, an, e-bike, col...\n",
       "7612    [the, latest, more, homes, razed, by, northern...\n",
       "Name: text, Length: 7606, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(df[df[\"text\"].isna()==True].index,inplace=True)\n",
    "df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "530dd5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [our, deeds, are, the, reason, of, this, earth...\n",
       "1               [forest, fire, near, la, sask, ., canada]\n",
       "2       [all, residents, asked, to, in, place, are, be...\n",
       "3       [13,000, people, receive, wildfires, evacuatio...\n",
       "4       [just, got, sent, this, photo, from, ruby, ala...\n",
       "                              ...                        \n",
       "421                                                   NaN\n",
       "433                                                   NaN\n",
       "823                                                   NaN\n",
       "5115                                                  NaN\n",
       "5331                                                  NaN\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64178345",
   "metadata": {},
   "source": [
    "### Words non existent in english dictionary, website links removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f38b2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srija\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "df[\"text_final\"]=\"\"\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "for index,entry in enumerate(df['clean_text']):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "#     print(index,entry)\n",
    "    for word, tag in pos_tag(entry):\n",
    "#         print(word, tag)\n",
    "#         Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha() and word!=\"http\" :\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "            \n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    \n",
    "#     print(Final_words)\n",
    "\n",
    "#     #getting bigrams\n",
    "#     bigram_list = []\n",
    "#     bigrams = ngrams(Final_words, 2)   \n",
    "#     for words in bigrams:\n",
    "#         bigram_list.append(combine_text(words))\n",
    "#     df.loc[index,\"bigram\"] = str(bigram_list)\n",
    "#     val=\"\"\n",
    "#     if Final_words!=[]:\n",
    "#         val = combine_text(Final_words)\n",
    "#     df.loc[index,'text_final'] = str(Final_words)\n",
    "    df[\"text_final\"].iloc[index]=str(Final_words)\n",
    "#     if len(Final_words)==0:\n",
    "#         print(index,df[\"text\"][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f443ec0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([], dtype='int64')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_text = df[df[\"text_final\"]==\"\"].index\n",
    "df.drop(empty_text,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cff3600b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "      <td>['deed', 'reason', 'earthquake', 'may', 'allah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, ., canada]</td>\n",
       "      <td>['forest', 'fire', 'near', 'la', 'ronge', 'sas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>['resident', 'ask', 'shelter', 'place', 'notif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[13,000, people, receive, wildfires, evacuatio...</td>\n",
       "      <td>['people', 'receive', 'wildfire', 'evacuation'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>['get', 'send', 'photo', 'ruby', 'alaska', 'sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7604</th>\n",
       "      <td>10863</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#WorldNews Fallen powerlines on G:link tram: U...</td>\n",
       "      <td>1</td>\n",
       "      <td>[worldnews, fallen, powerlines, on, g, :, link...</td>\n",
       "      <td>['worldnews', 'fall', 'powerlines', 'g', 'link...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>10864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>on the flip side I'm at Walmart and there is a...</td>\n",
       "      <td>1</td>\n",
       "      <td>[on, the, flip, side, im, at, walmart, and, th...</td>\n",
       "      <td>['flip', 'side', 'im', 'walmart', 'bomb', 'eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7606</th>\n",
       "      <td>10866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Suicide bomber kills 15 in Saudi security site...</td>\n",
       "      <td>1</td>\n",
       "      <td>[suicide, bomber, kills, 15, in, saudi, securi...</td>\n",
       "      <td>['suicide', 'bomber', 'kill', 'saudi', 'securi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>[two, giant, cranes, holding, a, bridge, colla...</td>\n",
       "      <td>['two', 'giant', 'crane', 'hold', 'bridge', 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, latest, :, more, homes, razed, by, north...</td>\n",
       "      <td>['late', 'home', 'raze', 'northern', 'californ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7503 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7604  10863     NaN      NaN   \n",
       "7605  10864     NaN      NaN   \n",
       "7606  10866     NaN      NaN   \n",
       "7608  10869     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7604  #WorldNews Fallen powerlines on G:link tram: U...       1   \n",
       "7605  on the flip side I'm at Walmart and there is a...       1   \n",
       "7606  Suicide bomber kills 15 in Saudi security site...       1   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "                                             clean_text  \\\n",
       "0     [our, deeds, are, the, reason, of, this, earth...   \n",
       "1      [forest, fire, near, la, ronge, sask, ., canada]   \n",
       "2     [all, residents, asked, to, shelter, in, place...   \n",
       "3     [13,000, people, receive, wildfires, evacuatio...   \n",
       "4     [just, got, sent, this, photo, from, ruby, ala...   \n",
       "...                                                 ...   \n",
       "7604  [worldnews, fallen, powerlines, on, g, :, link...   \n",
       "7605  [on, the, flip, side, im, at, walmart, and, th...   \n",
       "7606  [suicide, bomber, kills, 15, in, saudi, securi...   \n",
       "7608  [two, giant, cranes, holding, a, bridge, colla...   \n",
       "7612  [the, latest, :, more, homes, razed, by, north...   \n",
       "\n",
       "                                             text_final  \n",
       "0     ['deed', 'reason', 'earthquake', 'may', 'allah...  \n",
       "1     ['forest', 'fire', 'near', 'la', 'ronge', 'sas...  \n",
       "2     ['resident', 'ask', 'shelter', 'place', 'notif...  \n",
       "3     ['people', 'receive', 'wildfire', 'evacuation'...  \n",
       "4     ['get', 'send', 'photo', 'ruby', 'alaska', 'sm...  \n",
       "...                                                 ...  \n",
       "7604  ['worldnews', 'fall', 'powerlines', 'g', 'link...  \n",
       "7605  ['flip', 'side', 'im', 'walmart', 'bomb', 'eve...  \n",
       "7606  ['suicide', 'bomber', 'kill', 'saudi', 'securi...  \n",
       "7608  ['two', 'giant', 'crane', 'hold', 'bridge', 'c...  \n",
       "7612  ['late', 'home', 'raze', 'northern', 'californ...  \n",
       "\n",
       "[7503 rows x 7 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2fe68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c327fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "889a8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"text\"] = [word_tokenize(entry) for entry in test['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5da1a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"text\"] = test[\"text\"].apply(lambda x:word_in_english_dictionary(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2c186bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = [entry.lower() for entry in test['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "afc075ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text']= [word_tokenize(entry) for entry in test['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4e3840b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "for index,entry in enumerate(test['text']):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha()  and word!=\"http\":\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    test.loc[index,'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "9db60ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[just, happened, a, terrible, car, crash]</td>\n",
       "      <td>['happen', 'terrible', 'car', 'crash']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[heard, about, #, earthquake, is, different, c...</td>\n",
       "      <td>['heard', 'earthquake', 'different', 'city', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[there, is, a, forest, fire, at, spot, pond, ,...</td>\n",
       "      <td>['forest', 'fire', 'spot', 'pond', 'geese', 'f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[apocalypse, lighting, ., #, spokane, #, wildf...</td>\n",
       "      <td>['apocalypse', 'light', 'spokane', 'wildfire']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[typhoon, soudelor, kills, 28, in, china, and,...</td>\n",
       "      <td>['typhoon', 'soudelor', 'kill', 'china', 'taiw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[earthquake, safety, los, angeles, Â‰Ã»Ã², safety...</td>\n",
       "      <td>['earthquake', 'safety', 'los', 'angeles', 'sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[storm, in, ri, worse, than, last, hurricane, ...</td>\n",
       "      <td>['storm', 'ri', 'bad', 'last', 'hurricane', 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[green, line, derailment, in, chicago, http, :...</td>\n",
       "      <td>['green', 'line', 'derailment', 'chicago']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[meg, issues, hazardous, weather, outlook, (, ...</td>\n",
       "      <td>['meg', 'issue', 'hazardous', 'weather', 'outl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[#, cityofcalgary, has, activated, its, munici...</td>\n",
       "      <td>['cityofcalgary', 'activate', 'municipal', 'em...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0     NaN      NaN   \n",
       "1         2     NaN      NaN   \n",
       "2         3     NaN      NaN   \n",
       "3         9     NaN      NaN   \n",
       "4        11     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                   text  \\\n",
       "0             [just, happened, a, terrible, car, crash]   \n",
       "1     [heard, about, #, earthquake, is, different, c...   \n",
       "2     [there, is, a, forest, fire, at, spot, pond, ,...   \n",
       "3     [apocalypse, lighting, ., #, spokane, #, wildf...   \n",
       "4     [typhoon, soudelor, kills, 28, in, china, and,...   \n",
       "...                                                 ...   \n",
       "3258  [earthquake, safety, los, angeles, Â‰Ã»Ã², safety...   \n",
       "3259  [storm, in, ri, worse, than, last, hurricane, ...   \n",
       "3260  [green, line, derailment, in, chicago, http, :...   \n",
       "3261  [meg, issues, hazardous, weather, outlook, (, ...   \n",
       "3262  [#, cityofcalgary, has, activated, its, munici...   \n",
       "\n",
       "                                             text_final  \n",
       "0                ['happen', 'terrible', 'car', 'crash']  \n",
       "1     ['heard', 'earthquake', 'different', 'city', '...  \n",
       "2     ['forest', 'fire', 'spot', 'pond', 'geese', 'f...  \n",
       "3        ['apocalypse', 'light', 'spokane', 'wildfire']  \n",
       "4     ['typhoon', 'soudelor', 'kill', 'china', 'taiw...  \n",
       "...                                                 ...  \n",
       "3258  ['earthquake', 'safety', 'los', 'angeles', 'sa...  \n",
       "3259  ['storm', 'ri', 'bad', 'last', 'hurricane', 'c...  \n",
       "3260         ['green', 'line', 'derailment', 'chicago']  \n",
       "3261  ['meg', 'issue', 'hazardous', 'weather', 'outl...  \n",
       "3262  ['cityofcalgary', 'activate', 'municipal', 'em...  \n",
       "\n",
       "[3263 rows x 5 columns]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "fa136136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          7613 non-null   int64 \n",
      " 1   keyword     7613 non-null   object\n",
      " 2   location    7613 non-null   object\n",
      " 3   text        7613 non-null   object\n",
      " 4   target      7613 non-null   int64 \n",
      " 5   text_final  7613 non-null   object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 357.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff2885",
   "metadata": {},
   "source": [
    "### Vectorizing: TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3f1f7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68f1c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "241bf2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>ab</th>\n",
       "      <th>aba</th>\n",
       "      <th>aba woman</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandon aircraft</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abbswinston</th>\n",
       "      <th>abbswinston zionist</th>\n",
       "      <th>abc</th>\n",
       "      <th>...</th>\n",
       "      <th>zippednews</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zipper bag</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombie apocalypse</th>\n",
       "      <th>zone</th>\n",
       "      <th>zone come</th>\n",
       "      <th>zone dont</th>\n",
       "      <th>zouma</th>\n",
       "      <th>zouma flatten</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7498</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7499</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7500</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7501</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7502</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.282141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7503 rows Ã— 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aa   ab  aba  aba woman  abandon  abandon aircraft  abbott  \\\n",
       "0     0.0  0.0  0.0        0.0      0.0               0.0     0.0   \n",
       "1     0.0  0.0  0.0        0.0      0.0               0.0     0.0   \n",
       "2     0.0  0.0  0.0        0.0      0.0               0.0     0.0   \n",
       "3     0.0  0.0  0.0        0.0      0.0               0.0     0.0   \n",
       "4     0.0  0.0  0.0        0.0      0.0               0.0     0.0   \n",
       "...   ...  ...  ...        ...      ...               ...     ...   \n",
       "7498  0.0  0.0  0.0        0.0      0.0               0.0     0.0   \n",
       "7499  0.0  0.0  0.0        0.0      0.0               0.0     0.0   \n",
       "7500  0.0  0.0  0.0        0.0      0.0               0.0     0.0   \n",
       "7501  0.0  0.0  0.0        0.0      0.0               0.0     0.0   \n",
       "7502  0.0  0.0  0.0        0.0      0.0               0.0     0.0   \n",
       "\n",
       "      abbswinston  abbswinston zionist       abc  ...  zippednews  zipper  \\\n",
       "0             0.0                  0.0  0.000000  ...         0.0     0.0   \n",
       "1             0.0                  0.0  0.000000  ...         0.0     0.0   \n",
       "2             0.0                  0.0  0.000000  ...         0.0     0.0   \n",
       "3             0.0                  0.0  0.000000  ...         0.0     0.0   \n",
       "4             0.0                  0.0  0.000000  ...         0.0     0.0   \n",
       "...           ...                  ...       ...  ...         ...     ...   \n",
       "7498          0.0                  0.0  0.000000  ...         0.0     0.0   \n",
       "7499          0.0                  0.0  0.000000  ...         0.0     0.0   \n",
       "7500          0.0                  0.0  0.000000  ...         0.0     0.0   \n",
       "7501          0.0                  0.0  0.000000  ...         0.0     0.0   \n",
       "7502          0.0                  0.0  0.282141  ...         0.0     0.0   \n",
       "\n",
       "      zipper bag  zombie  zombie apocalypse  zone  zone come  zone dont  \\\n",
       "0            0.0     0.0                0.0   0.0        0.0        0.0   \n",
       "1            0.0     0.0                0.0   0.0        0.0        0.0   \n",
       "2            0.0     0.0                0.0   0.0        0.0        0.0   \n",
       "3            0.0     0.0                0.0   0.0        0.0        0.0   \n",
       "4            0.0     0.0                0.0   0.0        0.0        0.0   \n",
       "...          ...     ...                ...   ...        ...        ...   \n",
       "7498         0.0     0.0                0.0   0.0        0.0        0.0   \n",
       "7499         0.0     0.0                0.0   0.0        0.0        0.0   \n",
       "7500         0.0     0.0                0.0   0.0        0.0        0.0   \n",
       "7501         0.0     0.0                0.0   0.0        0.0        0.0   \n",
       "7502         0.0     0.0                0.0   0.0        0.0        0.0   \n",
       "\n",
       "      zouma  zouma flatten  \n",
       "0       0.0            0.0  \n",
       "1       0.0            0.0  \n",
       "2       0.0            0.0  \n",
       "3       0.0            0.0  \n",
       "4       0.0            0.0  \n",
       "...     ...            ...  \n",
       "7498    0.0            0.0  \n",
       "7499    0.0            0.0  \n",
       "7500    0.0            0.0  \n",
       "7501    0.0            0.0  \n",
       "7502    0.0            0.0  \n",
       "\n",
       "[7503 rows x 10000 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=10000)\n",
    "X = vectorizer.fit_transform(df[\"text_final\"])\n",
    "tfidf_tokens = vectorizer.get_feature_names()\n",
    "df_tfidfvect = pd.DataFrame(data = X.toarray(),columns = tfidf_tokens)\n",
    "df_tfidfvect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9700cae8",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "#### 1.Tokenize the sentences into words.\n",
    "#### 2.Create one-hot encoded vector for each word.\n",
    "#### 3.Use padding to ensure all sequences are of same length.\n",
    "#### 4.Pass the padded sequences as input to embedding layer.\n",
    "#### 5.Flatten and apply Dense layer to predict the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10b2aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten,Embedding,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8129d312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7399, 2359, 7385, 2507, 6219, 5209, 7776],\n",
       " [2943, 8710, 9591, 1101, 5547, 5890, 9207],\n",
       " [9101, 8554, 4818, 582, 1827, 7507, 765, 4818, 582, 431, 2571],\n",
       " [9508, 7518, 4696, 765, 431, 3255],\n",
       " [5965, 9478, 270, 8227, 1353, 5348, 4696, 53, 7995],\n",
       " [8869, 9950, 3255, 2061, 5000, 1521, 5475, 3, 9441, 8710, 550, 4696],\n",
       " [3352, 7989, 9304, 6728, 5378, 2044, 3192, 6026, 7897, 1450, 5346, 6477],\n",
       " [8585, 5554, 249, 9491, 8710, 2542],\n",
       " [8342, 3176, 765, 676, 6347, 4761, 6026],\n",
       " [8585, 4084, 5878, 9900, 6477]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab_size=10000\n",
    "encoded_tweets = [one_hot(d,Vocab_size) for d in df[\"text_final\"].values]\n",
    "encoded_tweets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60b0c55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"['deed', 'reason', 'earthquake', 'may', 'allah', 'forgive', 'u']\",\n",
       "       \"['forest', 'fire', 'near', 'la', 'ronge', 'sask', 'canada']\",\n",
       "       \"['resident', 'ask', 'shelter', 'place', 'notify', 'officer', 'evacuation', 'shelter', 'place', 'order', 'expect']\",\n",
       "       \"['people', 'receive', 'wildfire', 'evacuation', 'order', 'california']\",\n",
       "       \"['get', 'send', 'photo', 'ruby', 'alaska', 'smoke', 'wildfire', 'pour', 'school']\",\n",
       "       \"['rockyfire', 'update', 'california', 'hwy', 'close', 'direction', 'due', 'lake', 'county', 'fire', 'cafire', 'wildfire']\",\n",
       "       \"['flood', 'disaster', 'heavy', 'rain', 'cause', 'flash', 'flooding', 'street', 'manitou', 'colorado', 'spring', 'area']\",\n",
       "       \"['im', 'top', 'hill', 'see', 'fire', 'wood']\",\n",
       "       \"['there', 'emergency', 'evacuation', 'happen', 'building', 'across', 'street']\",\n",
       "       \"['im', 'afraid', 'tornado', 'come', 'area']\"], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text_final\"].values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1ae1995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-e9079aa10fcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mencoded_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"i\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpadded_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpadded_reviews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m    150\u001b[0m           \u001b[1;32mor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcase\u001b[0m \u001b[0mof\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m   \"\"\"\n\u001b[1;32m--> 152\u001b[1;33m   return sequence.pad_sequences(\n\u001b[0m\u001b[0;32m    153\u001b[0m       \u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m       padding=padding, truncating=truncating, value=value)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m     83\u001b[0m                          .format(dtype, type(value)))\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mfull\u001b[1;34m(shape, fill_value, dtype, order)\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m     \u001b[0mmultiarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unsafe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "max_length = max( (x) for x in encoded_tweets[:100])\n",
    "padded_tweets = pad_sequences(encoded_tweets[:100],maxlen=max_length,padding='post')\n",
    "padded_reviews[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "1ec80d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=[ 'word_count', 'url_count', 'mean_word_length','punctuation_count', 'hashtag_count', 'mention_count']\n",
    "df.shape,df_tfidfvect.shape\n",
    "d = df[df[\"text_final\"].isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "97700d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>ab</th>\n",
       "      <th>aba</th>\n",
       "      <th>aba woman</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandon aircraft</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abc</th>\n",
       "      <th>abc news</th>\n",
       "      <th>abc online</th>\n",
       "      <th>...</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombie apocalypse</th>\n",
       "      <th>zone</th>\n",
       "      <th>zone come</th>\n",
       "      <th>word_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.384615</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.090909</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.125000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7491</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.166667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7492</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.692308</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7493</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.538462</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7494</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.052632</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7495</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.833333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7460 rows Ã— 10006 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aa   ab  aba  aba woman  abandon  abandon aircraft  abbott  abc  \\\n",
       "0     0.0  0.0  0.0        0.0      0.0               0.0     0.0  0.0   \n",
       "1     0.0  0.0  0.0        0.0      0.0               0.0     0.0  0.0   \n",
       "2     0.0  0.0  0.0        0.0      0.0               0.0     0.0  0.0   \n",
       "3     0.0  0.0  0.0        0.0      0.0               0.0     0.0  0.0   \n",
       "4     0.0  0.0  0.0        0.0      0.0               0.0     0.0  0.0   \n",
       "...   ...  ...  ...        ...      ...               ...     ...  ...   \n",
       "7491  NaN  NaN  NaN        NaN      NaN               NaN     NaN  NaN   \n",
       "7492  NaN  NaN  NaN        NaN      NaN               NaN     NaN  NaN   \n",
       "7493  NaN  NaN  NaN        NaN      NaN               NaN     NaN  NaN   \n",
       "7494  NaN  NaN  NaN        NaN      NaN               NaN     NaN  NaN   \n",
       "7495  NaN  NaN  NaN        NaN      NaN               NaN     NaN  NaN   \n",
       "\n",
       "      abc news  abc online  ...  zombie  zombie apocalypse  zone  zone come  \\\n",
       "0          0.0         0.0  ...     0.0                0.0   0.0        0.0   \n",
       "1          0.0         0.0  ...     0.0                0.0   0.0        0.0   \n",
       "2          0.0         0.0  ...     0.0                0.0   0.0        0.0   \n",
       "3          0.0         0.0  ...     0.0                0.0   0.0        0.0   \n",
       "4          0.0         0.0  ...     0.0                0.0   0.0        0.0   \n",
       "...        ...         ...  ...     ...                ...   ...        ...   \n",
       "7491       NaN         NaN  ...     NaN                NaN   NaN        NaN   \n",
       "7492       NaN         NaN  ...     NaN                NaN   NaN        NaN   \n",
       "7493       NaN         NaN  ...     NaN                NaN   NaN        NaN   \n",
       "7494       NaN         NaN  ...     NaN                NaN   NaN        NaN   \n",
       "7495       NaN         NaN  ...     NaN                NaN   NaN        NaN   \n",
       "\n",
       "      word_count  url_count  mean_word_length  punctuation_count  \\\n",
       "0           13.0        0.0          4.384615                1.0   \n",
       "1            7.0        0.0          4.571429                1.0   \n",
       "2           22.0        0.0          5.090909                3.0   \n",
       "3            8.0        0.0          7.125000                2.0   \n",
       "4           16.0        0.0          4.500000                2.0   \n",
       "...          ...        ...               ...                ...   \n",
       "7491        12.0        0.0          6.166667                3.0   \n",
       "7492        26.0        0.0          3.692308                7.0   \n",
       "7493        13.0        0.0          7.538462                5.0   \n",
       "7494        19.0        1.0          5.052632                8.0   \n",
       "7495         6.0        1.0          6.833333                5.0   \n",
       "\n",
       "      hashtag_count  mention_count  \n",
       "0               1.0            0.0  \n",
       "1               0.0            0.0  \n",
       "2               0.0            0.0  \n",
       "3               1.0            0.0  \n",
       "4               2.0            0.0  \n",
       "...             ...            ...  \n",
       "7491            1.0            0.0  \n",
       "7492            0.0            0.0  \n",
       "7493            1.0            3.0  \n",
       "7494            0.0            0.0  \n",
       "7495            0.0            0.0  \n",
       "\n",
       "[7460 rows x 10006 columns]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final = pd.concat([df_tfidfvect, d[cols]],axis=1)\n",
    "df_final.drop_duplicates(subset=None, keep='first', inplace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e333e617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5307, (7613, 5300))"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c61d5abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aba</th>\n",
       "      <th>aba woman</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandon aircraft</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abc</th>\n",
       "      <th>abc news</th>\n",
       "      <th>abe</th>\n",
       "      <th>ability</th>\n",
       "      <th>...</th>\n",
       "      <th>zone come</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>stop_word_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>char_count</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4.384615</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>5.090909</td>\n",
       "      <td>133</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.125000</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>88</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6.636364</td>\n",
       "      <td>83</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>125</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>65</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6.263158</td>\n",
       "      <td>137</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.279142</td>\n",
       "      <td>0.296183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6.307692</td>\n",
       "      <td>94</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 5299 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aa  aba  aba woman  abandon  abandon aircraft  abbott       abc  \\\n",
       "0     0.0  0.0        0.0      0.0               0.0     0.0  0.000000   \n",
       "1     0.0  0.0        0.0      0.0               0.0     0.0  0.000000   \n",
       "2     0.0  0.0        0.0      0.0               0.0     0.0  0.000000   \n",
       "3     0.0  0.0        0.0      0.0               0.0     0.0  0.000000   \n",
       "4     0.0  0.0        0.0      0.0               0.0     0.0  0.000000   \n",
       "...   ...  ...        ...      ...               ...     ...       ...   \n",
       "7608  0.0  0.0        0.0      0.0               0.0     0.0  0.000000   \n",
       "7609  0.0  0.0        0.0      0.0               0.0     0.0  0.000000   \n",
       "7610  0.0  0.0        0.0      0.0               0.0     0.0  0.000000   \n",
       "7611  0.0  0.0        0.0      0.0               0.0     0.0  0.000000   \n",
       "7612  0.0  0.0        0.0      0.0               0.0     0.0  0.279142   \n",
       "\n",
       "      abc news  abe  ability  ...  zone come  word_count  unique_word_count  \\\n",
       "0     0.000000  0.0      0.0  ...        0.0          13                 13   \n",
       "1     0.000000  0.0      0.0  ...        0.0           7                  7   \n",
       "2     0.000000  0.0      0.0  ...        0.0          22                 20   \n",
       "3     0.000000  0.0      0.0  ...        0.0           8                  8   \n",
       "4     0.000000  0.0      0.0  ...        0.0          16                 15   \n",
       "...        ...  ...      ...  ...        ...         ...                ...   \n",
       "7608  0.000000  0.0      0.0  ...        0.0          11                 11   \n",
       "7609  0.000000  0.0      0.0  ...        0.0          20                 17   \n",
       "7610  0.000000  0.0      0.0  ...        0.0           8                  8   \n",
       "7611  0.000000  0.0      0.0  ...        0.0          19                 19   \n",
       "7612  0.296183  0.0      0.0  ...        0.0          13                 13   \n",
       "\n",
       "      stop_word_count  url_count  mean_word_length  char_count  \\\n",
       "0                   6          0          4.384615          69   \n",
       "1                   0          0          4.571429          38   \n",
       "2                  11          0          5.090909         133   \n",
       "3                   1          0          7.125000          65   \n",
       "4                   7          0          4.500000          88   \n",
       "...               ...        ...               ...         ...   \n",
       "7608                2          1          6.636364          83   \n",
       "7609                9          0          5.300000         125   \n",
       "7610                2          1          7.250000          65   \n",
       "7611                5          0          6.263158         137   \n",
       "7612                3          1          6.307692          94   \n",
       "\n",
       "      punctuation_count  hashtag_count  mention_count  \n",
       "0                     1              1              0  \n",
       "1                     1              0              0  \n",
       "2                     3              0              0  \n",
       "3                     2              1              0  \n",
       "4                     2              2              0  \n",
       "...                 ...            ...            ...  \n",
       "7608                  5              0              0  \n",
       "7609                  5              0              2  \n",
       "7610                 11              0              0  \n",
       "7611                  5              0              0  \n",
       "7612                  7              0              0  \n",
       "\n",
       "[7613 rows x 5299 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_final.drop([\"bigram\",\"text_final\",\"text_space\",\"target\",\"text\"], axis=1, inplace=True)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb1b27a",
   "metadata": {},
   "source": [
    "### Other Vectorizing approach: Countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5ef3c4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aa battery</th>\n",
       "      <th>aa mgm</th>\n",
       "      <th>aa plan</th>\n",
       "      <th>ab</th>\n",
       "      <th>ab resin</th>\n",
       "      <th>aba</th>\n",
       "      <th>aba woman</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandon aircraft</th>\n",
       "      <th>...</th>\n",
       "      <th>zone move</th>\n",
       "      <th>zone outside</th>\n",
       "      <th>zone power</th>\n",
       "      <th>zone rove</th>\n",
       "      <th>zone thank</th>\n",
       "      <th>zone war</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoom one</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zurich swiss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 43944 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aa  aa battery  aa mgm  aa plan  ab  ab resin  aba  aba woman  abandon  \\\n",
       "0      0           0       0        0   0         0    0          0        0   \n",
       "1      0           0       0        0   0         0    0          0        0   \n",
       "2      0           0       0        0   0         0    0          0        0   \n",
       "3      0           0       0        0   0         0    0          0        0   \n",
       "4      0           0       0        0   0         0    0          0        0   \n",
       "...   ..         ...     ...      ...  ..       ...  ...        ...      ...   \n",
       "7608   0           0       0        0   0         0    0          0        0   \n",
       "7609   0           0       0        0   0         0    0          0        0   \n",
       "7610   0           0       0        0   0         0    0          0        0   \n",
       "7611   0           0       0        0   0         0    0          0        0   \n",
       "7612   0           0       0        0   0         0    0          0        0   \n",
       "\n",
       "      abandon aircraft  ...  zone move  zone outside  zone power  zone rove  \\\n",
       "0                    0  ...          0             0           0          0   \n",
       "1                    0  ...          0             0           0          0   \n",
       "2                    0  ...          0             0           0          0   \n",
       "3                    0  ...          0             0           0          0   \n",
       "4                    0  ...          0             0           0          0   \n",
       "...                ...  ...        ...           ...         ...        ...   \n",
       "7608                 0  ...          0             0           0          0   \n",
       "7609                 0  ...          0             0           0          0   \n",
       "7610                 0  ...          0             0           0          0   \n",
       "7611                 0  ...          0             0           0          0   \n",
       "7612                 0  ...          0             0           0          0   \n",
       "\n",
       "      zone thank  zone war  zoom  zoom one  zurich  zurich swiss  \n",
       "0              0         0     0         0       0             0  \n",
       "1              0         0     0         0       0             0  \n",
       "2              0         0     0         0       0             0  \n",
       "3              0         0     0         0       0             0  \n",
       "4              0         0     0         0       0             0  \n",
       "...          ...       ...   ...       ...     ...           ...  \n",
       "7608           0         0     0         0       0             0  \n",
       "7609           0         0     0         0       0             0  \n",
       "7610           0         0     0         0       0             0  \n",
       "7611           0         0     0         0       0             0  \n",
       "7612           0         0     0         0       0             0  \n",
       "\n",
       "[7613 rows x 43944 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range= (1, 2))\n",
    "X = vectorizer.fit_transform(df[\"text_final\"])\n",
    "df_countvec = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())\n",
    "df_countvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "705e86af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['speed', 'among', 'top', 'cause', 'teen', 'accident', 'car', 'accident']\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_countvec[df_countvec[\"accident\"]>1]\n",
    "df.iloc[71][\"text_final\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "342c5dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['aa', 'aba', 'aba woman', 'abandon', 'abandon aircraft', 'abbott',\n",
       "       'abc', 'abc news', 'abe', 'ability',\n",
       "       ...\n",
       "       'youtube playlist', 'youtube video', 'yr', 'zero', 'zionist',\n",
       "       'zionist terrorist', 'zombie', 'zombie apocalypse', 'zone',\n",
       "       'zone come'],\n",
       "      dtype='object', length=5290)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvec_measure = df_countvec.sum()\n",
    "countvec_measure\n",
    "indeces = countvec_measure[countvec_measure>2].index\n",
    "indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "2f0ba594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_array = np.array(tfidf_tokens)\n",
    "# tfidf_sorting = np.argsort(vectorizer.transform(all_words[:15000]).toarray()).flatten()[::-1]\n",
    "\n",
    "# n = 10000\n",
    "\n",
    "# top_n = feature_array[tfidf_sorting][:n]\n",
    "# top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "104c31ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ['deed', 'reason', 'earthquake', 'may', 'allah...\n",
       "1       ['forest', 'fire', 'near', 'la', 'sask', 'cana...\n",
       "2       ['resident', 'ask', 'place', 'notify', 'office...\n",
       "3       ['people', 'receive', 'wildfire', 'evacuation'...\n",
       "4       ['get', 'send', 'photo', 'ruby', 'alaska', 'sm...\n",
       "                              ...                        \n",
       "7608    ['two', 'giant', 'crane', 'hold', 'bridge', 'c...\n",
       "7609    ['control', 'wild', 'fire', 'california', 'eve...\n",
       "7610                         ['utc', 'volcano', 'hawaii']\n",
       "7611    ['police', 'investigate', 'collided', 'car', '...\n",
       "7612    ['late', 'home', 'raze', 'northern', 'californ...\n",
       "Name: text_final, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text_final\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "5409f4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9001274762176957, 0.45006373810884787, 2.179506868842596)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidfvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "0dd78766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['zimmerman fan', 'zimmerman zimmerman', 'zionism', 'zionism stop',\n",
       "       'zionism zionism', 'zionist', 'zionist terrorist', 'zionist zionist',\n",
       "       'zionists', 'zionists revere', 'zionists zionists', 'zip', 'zip body',\n",
       "       'zip po', 'zip zip', 'zipper', 'zipper bag', 'zipper zipper', 'zodiac',\n",
       "       'zodiac girl', 'zombie', 'zombie apocalypse', 'zombie feel',\n",
       "       'zombie fun', 'zombie take', 'zombie zombie', 'zone', 'zone alaska',\n",
       "       'zone come', 'zone common', 'zone cut', 'zone dust', 'zone egg',\n",
       "       'zone everyone', 'zone god', 'zone homies', 'zone johnny', 'zone move',\n",
       "       'zone outside', 'zone power', 'zone rove', 'zone thank', 'zone war',\n",
       "       'zone zone', 'zoom', 'zoom one', 'zoom zoom', 'zurich', 'zurich swiss',\n",
       "       'zurich zurich'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidfvect.columns[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "27c85c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"['deed', 'reason', 'earthquake', 'may', 'allah', 'forgive', 'u']\",\n",
       " \"['forest', 'fire', 'near', 'la', 'sask', 'canada']\")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_final'][0],df['text_final'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "82c7cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(df_tfidfvect,df['target'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "3812a17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>ab</th>\n",
       "      <th>aba</th>\n",
       "      <th>aba woman</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandon aircraft</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abc</th>\n",
       "      <th>abc news</th>\n",
       "      <th>abc online</th>\n",
       "      <th>...</th>\n",
       "      <th>zionism</th>\n",
       "      <th>zionist</th>\n",
       "      <th>zionist terrorist</th>\n",
       "      <th>zip</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zipper bag</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombie apocalypse</th>\n",
       "      <th>zone</th>\n",
       "      <th>zone come</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4502</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6578</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6822</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4064</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4073</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6156</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3257</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5908 rows Ã— 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aa   ab  aba  aba woman  abandon  abandon aircraft  abbott  abc  \\\n",
       "4502  0.0  0.0  0.0        0.0      0.0               0.0     0.0  0.0   \n",
       "1039  0.0  0.0  0.0        0.0      0.0               0.0     0.0  0.0   \n",
       "6578  0.0  0.0  0.0        0.0      0.0               0.0     0.0  0.0   \n",
       "6822  0.0  0.0  0.0        0.0      0.0               0.0     0.0  0.0   \n",
       "4064  0.0  0.0  0.0        0.0      0.0               0.0     0.0  0.0   \n",
       "...   ...  ...  ...        ...      ...               ...     ...  ...   \n",
       "4073  0.0  0.0  0.0        0.0      0.0               0.0     0.0  0.0   \n",
       "6156  0.0  0.0  0.0        0.0      0.0               0.0     0.0  0.0   \n",
       "3257  0.0  0.0  0.0        0.0      0.0               0.0     0.0  0.0   \n",
       "23    0.0  0.0  0.0        0.0      0.0               0.0     0.0  0.0   \n",
       "1014  0.0  0.0  0.0        0.0      0.0               0.0     0.0  0.0   \n",
       "\n",
       "      abc news  abc online  ...  zionism  zionist  zionist terrorist  zip  \\\n",
       "4502       0.0         0.0  ...      0.0      0.0                0.0  0.0   \n",
       "1039       0.0         0.0  ...      0.0      0.0                0.0  0.0   \n",
       "6578       0.0         0.0  ...      0.0      0.0                0.0  0.0   \n",
       "6822       0.0         0.0  ...      0.0      0.0                0.0  0.0   \n",
       "4064       0.0         0.0  ...      0.0      0.0                0.0  0.0   \n",
       "...        ...         ...  ...      ...      ...                ...  ...   \n",
       "4073       0.0         0.0  ...      0.0      0.0                0.0  0.0   \n",
       "6156       0.0         0.0  ...      0.0      0.0                0.0  0.0   \n",
       "3257       0.0         0.0  ...      0.0      0.0                0.0  0.0   \n",
       "23         0.0         0.0  ...      0.0      0.0                0.0  0.0   \n",
       "1014       0.0         0.0  ...      0.0      0.0                0.0  0.0   \n",
       "\n",
       "      zipper  zipper bag  zombie  zombie apocalypse  zone  zone come  \n",
       "4502     0.0         0.0     0.0                0.0   0.0        0.0  \n",
       "1039     0.0         0.0     0.0                0.0   0.0        0.0  \n",
       "6578     0.0         0.0     0.0                0.0   0.0        0.0  \n",
       "6822     0.0         0.0     0.0                0.0   0.0        0.0  \n",
       "4064     0.0         0.0     0.0                0.0   0.0        0.0  \n",
       "...      ...         ...     ...                ...   ...        ...  \n",
       "4073     0.0         0.0     0.0                0.0   0.0        0.0  \n",
       "6156     0.0         0.0     0.0                0.0   0.0        0.0  \n",
       "3257     0.0         0.0     0.0                0.0   0.0        0.0  \n",
       "23       0.0         0.0     0.0                0.0   0.0        0.0  \n",
       "1014     0.0         0.0     0.0                0.0   0.0        0.0  \n",
       "\n",
       "[5908 rows x 10000 columns]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "15c57c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = test[[\"text_final\"]]\n",
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "eb7ff6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6002,)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y)\n",
    "Test_Y = Encoder.fit_transform(Test_Y)\n",
    "Train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "397a6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "# Tfidf_vect.fit(df['text_final'])\n",
    "# Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "# Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "# X_test_Tfidf = Tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "100d031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8139d15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  80.4796802131912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.79600333, 0.80266445, 0.80166667, 0.7875    , 0.79      ])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X,Train_Y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(Test_X)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \", accuracy_score(predictions_NB, Test_Y)*100)\n",
    "cross_val_score(Naive, Train_X, Train_Y, cv=5)\n",
    "# predictions_NB = Naive.predict(X_test_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ea3c25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  80.3676953381484\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X,Train_Y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "# predictions_SVM\n",
    "\n",
    "# predictions_SVM = SVM.predict(X_test_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7c03e3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Accuracy Score ->  81.21252498334444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.80016653, 0.79600333, 0.79916667, 0.79083333, 0.795     ])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LogisticRegression = LogisticRegression(random_state = 0)\n",
    "LogisticRegression.fit(Train_X,Train_Y)\n",
    "lr = LogisticRegression.predict(Test_X)\n",
    "print(\"LogisticRegression Accuracy Score -> \",accuracy_score(lr, Test_Y)*100)\n",
    "cross_val_score(LogisticRegression, Train_X, Train_Y, cv=5)\n",
    "# predictions_lr = LogisticRegression.predict(X_test_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "22520955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier Accuracy Score ->  65.62291805463025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.65778518, 0.61032473, 0.65916667, 0.63333333, 0.64916667])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "KNeighborsClassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "KNeighborsClassifier.fit(Train_X,Train_Y)\n",
    "knn = KNeighborsClassifier.predict(Test_X)\n",
    "print(\"KNeighborsClassifier Accuracy Score -> \",accuracy_score(knn, Test_Y)*100)\n",
    "cross_val_score(KNeighborsClassifier, Train_X, Train_Y, cv=5)\n",
    "# knn = KNeighborsClassifier.predict(X_test_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ff15aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cebc1c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# c3 = GaussianNB()\n",
    "# c3.fit(Train_X_Tfidf,Train_Y)\n",
    "# y_pred = c3.predict(Test_X_Tfidf)\n",
    "# print(\"GaussianNB Accuracy Score -> \",accuracy_score(y_pred, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "47155d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier acc score/ ->  75.21652231845437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.74771024, 0.72189842, 0.70416667, 0.72416667, 0.73      ])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DecisionTreeClassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "DecisionTreeClassifier.fit(Train_X,Train_Y)\n",
    "dt = DecisionTreeClassifier.predict(Test_X)\n",
    "print(\"DecisionTreeClassifier acc score/ -> \",accuracy_score(dt, Test_Y)*100)\n",
    "cross_val_score(DecisionTreeClassifier, Train_X,Train_Y, cv=5)\n",
    "# dt = DecisionTreeClassifier.predict(X_test_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e084ef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier Accuracy Score ->  77.81479013990673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.79766861, 0.76602831, 0.7875    , 0.77833333, 0.7775    ])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForestClassifier = RandomForestClassifier(n_estimators = 40, criterion = 'entropy', random_state = 0)\n",
    "RandomForestClassifier.fit(Train_X,Train_Y)\n",
    "y_pred = RandomForestClassifier.predict(Test_X)\n",
    "print(\"RandomForestClassifier Accuracy Score -> \",accuracy_score(y_pred, Test_Y)*100)\n",
    "cross_val_score(RandomForestClassifier, Train_X,Train_Y, cv=5)\n",
    "# rf = RandomForestClassifier.predict(X_test_Tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cb5e6d",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8f30a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models\n",
    "# ridge = linear_model.Ridge()\n",
    "# lasso = linear_model.Lasso()\n",
    "# elastic = linear_model.ElasticNet()\n",
    "# lasso_lars = linear_model.LassoLars()\n",
    "# bayesian_ridge = linear_model.BayesianRidge()\n",
    "# logistic = LogisticRegression(random_state = 0)\n",
    "# sgd = linear_model.SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c30bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "penalty = ['l1', 'l2']\n",
    "C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "class_weight = [{1:0.5, 0:0.5}]\n",
    "solver = ['liblinear', 'saga']\n",
    "\n",
    "param_grid = dict(penalty=penalty,\n",
    "                  C=C,\n",
    "                  class_weight=class_weight,\n",
    "                  solver=solver)\n",
    "\n",
    "grid = GridSearchCV(estimator=LogisticRegression,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='roc_auc',\n",
    "                    verbose=1,\n",
    "                    n_jobs=-1)\n",
    "grid_result = grid.fit(Train_X, Train_Y)\n",
    "\n",
    "# random = RandomizedSearchCV(estimator=LogisticRegression,\n",
    "#                             param_distributions=param_distributions,\n",
    "#                             scoring='roc_auc',\n",
    "#                             verbose=1, n_jobs=-1,\n",
    "#                             n_iter=1000)\n",
    "# random_result = random.fit(Train_X, Train_Y)\n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7180f29c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "222d89ac",
   "metadata": {},
   "source": [
    "### Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8dece1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0ab61183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 0, 1, 1], dtype=int64),\n",
       " array([0, 0, 0, ..., 0, 1, 0], dtype=int64),\n",
       " \"['finally', 'demolish', 'spring', 'property', 'sit', 'vacant', 'since']\")"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred,Test_Y,df[\"text_final\"][2313]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "4edaf26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id  target\n",
       "0         0       1\n",
       "1         2       1\n",
       "2         3       1\n",
       "3         9       1\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       1\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       0\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.DataFrame({\"Id\":test[\"id\"],\n",
    "                       \"target\":(predictions_SVM)})\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "ce445d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   Id      3263 non-null   int64\n",
      " 1   target  3263 non-null   int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 51.1 KB\n"
     ]
    }
   ],
   "source": [
    "pred_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "bd787ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(\"export.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"export.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed18338e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3093749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
